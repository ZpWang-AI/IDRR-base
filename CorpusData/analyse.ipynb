{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDTB 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "usecols = [\n",
    "    'Relation', 'Section', \n",
    "    'Arg1_RawText', 'Arg2_RawText', \n",
    "    'Conn1', 'Conn2',\n",
    "    'ConnHeadSemClass1', 'ConnHeadSemClass2',\n",
    "    'Conn2SemClass1', 'Conn2SemClass2'\n",
    "]\n",
    "df = pd.read_csv(\n",
    "    r'D:\\0--data\\projects\\04.01-IDRR数据\\IDRR-base\\CorpusData\\PDTB2\\pdtb2.csv',\n",
    "    low_memory=False,\n",
    "    usecols=usecols,\n",
    ")\n",
    "# print(data.columns)\n",
    "\n",
    "# for k, v in data.iloc[0].to_dict().items():\n",
    "#     print(k, v)\n",
    "\n",
    "rename_map = {\n",
    "    'Arg1_RawText': 'Arg1',\n",
    "    'Arg2_RawText': 'Arg2',\n",
    "    'ConnHeadSemClass1': 'Conn1Sem1',\n",
    "    'ConnHeadSemClass2': 'Conn1Sem2',\n",
    "    'Conn2SemClass1': 'Conn2Sem1',\n",
    "    'Conn2SemClass2': 'Conn2Sem2',\n",
    "}\n",
    "df.rename(columns=rename_map, inplace=True)\n",
    "init_df = df.copy()\n",
    "# # print(df.columns)\n",
    "\n",
    "df = df[df['Relation']=='Implicit']\n",
    "\n",
    "train_df = df[~df['Section'].isin([0, 1, 21, 22, 23, 24])]\n",
    "dev_df = df[df['Section'].isin([0, 1])]\n",
    "test_df = df[df['Section'].isin([21, 22])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comparison',\n",
       " 'Comparison.Concession',\n",
       " 'Comparison.Concession.Contra-expectation',\n",
       " 'Comparison.Concession.Expectation',\n",
       " 'Comparison.Contrast',\n",
       " 'Comparison.Contrast.Juxtaposition',\n",
       " 'Comparison.Contrast.Opposition',\n",
       " 'Comparison.Pragmatic concession',\n",
       " 'Comparison.Pragmatic contrast',\n",
       " 'Contingency',\n",
       " 'Contingency.Cause',\n",
       " 'Contingency.Cause.Reason',\n",
       " 'Contingency.Cause.Result',\n",
       " 'Contingency.Condition.Hypothetical',\n",
       " 'Contingency.Pragmatic cause.Justification',\n",
       " 'Contingency.Pragmatic condition.Relevance',\n",
       " 'Expansion',\n",
       " 'Expansion.Alternative',\n",
       " 'Expansion.Alternative.Chosen alternative',\n",
       " 'Expansion.Alternative.Conjunctive',\n",
       " 'Expansion.Conjunction',\n",
       " 'Expansion.Exception',\n",
       " 'Expansion.Instantiation',\n",
       " 'Expansion.List',\n",
       " 'Expansion.Restatement',\n",
       " 'Expansion.Restatement.Equivalence',\n",
       " 'Expansion.Restatement.Generalization',\n",
       " 'Expansion.Restatement.Specification',\n",
       " 'Temporal',\n",
       " 'Temporal.Asynchronous.Precedence',\n",
       " 'Temporal.Asynchronous.Succession',\n",
       " 'Temporal.Synchrony']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df['Conn1Sem1'].unique().tolist()\n",
    "sorted(labels)\n",
    "# labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一级多标签统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of level1 labels\n",
      "{\n",
      "  \"Temporal\": {\n",
      "    \"\": 579,\n",
      "    \"Expansion\": 54,\n",
      "    \"Contingency\": 19,\n",
      "    \"Comparison\": 13,\n",
      "    \"Temporal\": 0\n",
      "  },\n",
      "  \"Comparison\": {\n",
      "    \"\": 1840,\n",
      "    \"Expansion\": 31,\n",
      "    \"Temporal\": 20,\n",
      "    \"Comparison\": 2,\n",
      "    \"Contingency\": 1\n",
      "  },\n",
      "  \"Contingency\": {\n",
      "    \"\": 3133,\n",
      "    \"Expansion\": 128,\n",
      "    \"Temporal\": 12,\n",
      "    \"Contingency\": 6,\n",
      "    \"Comparison\": 3\n",
      "  },\n",
      "  \"Expansion\": {\n",
      "    \"\": 6652,\n",
      "    \"Temporal\": 63,\n",
      "    \"Contingency\": 39,\n",
      "    \"Comparison\": 32,\n",
      "    \"Expansion\": 6\n",
      "  }\n",
      "}\n",
      "\n",
      "rank of level1 labels\n",
      "{\n",
      "  \"Temporal\": [\n",
      "    \"Comparison\",\n",
      "    \"Contingency\",\n",
      "    \"Expansion\"\n",
      "  ],\n",
      "  \"Comparison\": [\n",
      "    \"Contingency\",\n",
      "    \"Temporal\",\n",
      "    \"Expansion\"\n",
      "  ],\n",
      "  \"Contingency\": [\n",
      "    \"Comparison\",\n",
      "    \"Temporal\",\n",
      "    \"Expansion\"\n",
      "  ],\n",
      "  \"Expansion\": [\n",
      "    \"Comparison\",\n",
      "    \"Contingency\",\n",
      "    \"Temporal\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def sort_dict(dct):\n",
    "    return dict(sorted(dct.items(), key=lambda x:-x[1]))\n",
    "    \n",
    "class analyse_level1_label:\n",
    "    labels = 'Temporal Comparison Contingency Expansion'.split()\n",
    "    \n",
    "    @classmethod\n",
    "    def str_to_label(cls, string):\n",
    "        return string.split('.')[0]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_item_labels(cls, item):\n",
    "        primary_label = cls.str_to_label(item.Conn1Sem1)\n",
    "        secondary_labels = []\n",
    "        for s in [item.Conn1Sem2, item.Conn2Sem1, item.Conn2Sem2]:\n",
    "            if not pd.isna(s):\n",
    "                secondary_labels.append(cls.str_to_label(s))\n",
    "        return primary_label, secondary_labels\n",
    "        \n",
    "    def __new__(cls, df, sort_res=True):\n",
    "        rec = {l:{ll:0 for ll in ['']+cls.labels}for l in cls.labels}\n",
    "        for p in range(df.shape[0]):\n",
    "            item = df.iloc[p]\n",
    "            primary_label, secondary_labels = cls.get_item_labels(item)\n",
    "            if not secondary_labels:\n",
    "                rec[primary_label][''] += 1\n",
    "            else:\n",
    "                for l in secondary_labels:\n",
    "                    rec[primary_label][l] += 1\n",
    "\n",
    "        if sort_res:\n",
    "            for k in rec:\n",
    "                rec[k] = sort_dict(rec[k])\n",
    "                \n",
    "        import json\n",
    "        rec_string = json.dumps(rec, ensure_ascii=False, indent=2)\n",
    "        print('num of level1 labels')\n",
    "        print(rec_string)\n",
    "        \n",
    "        rank = {}\n",
    "        for k in rec:\n",
    "            vals = sorted(cls.labels, key=lambda x:rec[k][x])\n",
    "            rank[k] = [v for v in vals if k != v]\n",
    "        rank_string = json.dumps(rank, ensure_ascii=False, indent=2)\n",
    "        print('\\nrank of level1 labels')\n",
    "        print(rank_string)\n",
    "        pass\n",
    "    \n",
    "\n",
    "analyse_level1_label(train_df)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDTB 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 10 fields in line 9309, saw 11\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\0--data\\projects\\04.01-IDRR数据\\IDRR-base\\CorpusData\\analyse.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/0--data/projects/04.01-IDRR%E6%95%B0%E6%8D%AE/IDRR-base/CorpusData/analyse.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/0--data/projects/04.01-IDRR%E6%95%B0%E6%8D%AE/IDRR-base/CorpusData/analyse.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m0--data\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mprojects\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m04.01-IDRR数据\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mIDRR-base\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mCorpusData\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mPDTB3\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mprocessed\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mtrain.tsv\u001b[39;49m\u001b[39m'\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/0--data/projects/04.01-IDRR%E6%95%B0%E6%8D%AE/IDRR-base/CorpusData/analyse.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df\n",
      "File \u001b[1;32md:\\applications\\miniconda3\\envs\\python_main\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\applications\\miniconda3\\envs\\python_main\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32md:\\applications\\miniconda3\\envs\\python_main\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\applications\\miniconda3\\envs\\python_main\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 10 fields in line 9309, saw 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'D:\\0--data\\projects\\04.01-IDRR数据\\IDRR-base\\CorpusData\\PDTB3\\processed\\train.tsv', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arg1': {'CharacterSpanList': [[9, 240]],\n",
       "  'RawText': 'In an Oct. 19 review of \"The Misanthrope\" at Chicago\\'s Goodman Theatre (\"Revitalized Classics Take the Stage in Windy City,\" Leisure & Arts), the role of Celimene, played by Kim Cattrall, was mistakenly attributed to Christina Haag',\n",
       "  'TokenList': [[9, 11, 0, 0, 0],\n",
       "   [12, 14, 1, 0, 1],\n",
       "   [15, 19, 2, 0, 2],\n",
       "   [20, 22, 3, 0, 3],\n",
       "   [23, 29, 4, 0, 4],\n",
       "   [30, 32, 5, 0, 5],\n",
       "   [33, 34, 6, 0, 6],\n",
       "   [34, 37, 7, 0, 7],\n",
       "   [38, 49, 8, 0, 8],\n",
       "   [49, 50, 9, 0, 9],\n",
       "   [51, 53, 10, 0, 10],\n",
       "   [54, 61, 11, 0, 11],\n",
       "   [61, 63, 12, 0, 12],\n",
       "   [64, 71, 13, 0, 13],\n",
       "   [72, 79, 14, 0, 14],\n",
       "   [80, 81, 15, 0, 15],\n",
       "   [81, 82, 16, 0, 16],\n",
       "   [82, 93, 17, 0, 17],\n",
       "   [94, 102, 18, 0, 18],\n",
       "   [103, 107, 19, 0, 19],\n",
       "   [108, 111, 20, 0, 20],\n",
       "   [112, 117, 21, 0, 21],\n",
       "   [118, 120, 22, 0, 22],\n",
       "   [121, 126, 23, 0, 23],\n",
       "   [127, 131, 24, 0, 24],\n",
       "   [131, 132, 25, 0, 25],\n",
       "   [132, 133, 26, 0, 26],\n",
       "   [134, 141, 27, 0, 27],\n",
       "   [142, 143, 28, 0, 28],\n",
       "   [144, 148, 29, 0, 29],\n",
       "   [148, 149, 30, 0, 30],\n",
       "   [149, 150, 31, 0, 31],\n",
       "   [151, 154, 32, 0, 32],\n",
       "   [155, 159, 33, 0, 33],\n",
       "   [160, 162, 34, 0, 34],\n",
       "   [163, 171, 35, 0, 35],\n",
       "   [171, 172, 36, 0, 36],\n",
       "   [173, 179, 37, 0, 37],\n",
       "   [180, 182, 38, 0, 38],\n",
       "   [183, 186, 39, 0, 39],\n",
       "   [187, 195, 40, 0, 40],\n",
       "   [195, 196, 41, 0, 41],\n",
       "   [197, 200, 42, 0, 42],\n",
       "   [201, 211, 43, 0, 43],\n",
       "   [212, 222, 44, 0, 44],\n",
       "   [223, 225, 45, 0, 45],\n",
       "   [226, 235, 46, 0, 46],\n",
       "   [236, 240, 47, 0, 47]]},\n",
       " 'Arg2': {'CharacterSpanList': [[242, 264]],\n",
       "  'RawText': 'Ms. Haag plays Elianti',\n",
       "  'TokenList': [[242, 245, 49, 1, 0],\n",
       "   [246, 250, 50, 1, 1],\n",
       "   [251, 256, 51, 1, 2],\n",
       "   [257, 264, 52, 1, 3]]},\n",
       " 'Connective': {'CharacterSpanList': [],\n",
       "  'RawText': 'however',\n",
       "  'TokenList': []},\n",
       " 'DocID': 'wsj_0200',\n",
       " 'ID': 3173,\n",
       " 'Sense': ['Comparison.Contrast'],\n",
       " 'Type': 'Implicit'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "train_file = r'D:\\0--data\\projects\\04.01-IDRR数据\\IDRR-base\\CorpusData\\2015-2016_conll_shared_task\\data\\conll16st-en-03-29-16-train\\relations.json'\n",
    "dev_file = r'D:\\0--data\\projects\\04.01-IDRR数据\\IDRR-base\\CorpusData\\2015-2016_conll_shared_task\\data\\conll16st-en-03-29-16-dev\\relations.json'\n",
    "test_file = r'D:\\0--data\\projects\\04.01-IDRR数据\\IDRR-base\\CorpusData\\2015-2016_conll_shared_task\\data\\conll16st-en-03-29-16-test\\relations.json'\n",
    "blind_test_file = r'D:\\0--data\\projects\\04.01-IDRR数据\\IDRR-base\\CorpusData\\2015-2016_conll_shared_task\\data\\conll16st-en-03-29-16-blind-test\\relations.json'\n",
    "\n",
    "def get_dicts(file):\n",
    "    # dicts = []\n",
    "    with open(file, 'r', encoding='utf8')as f:\n",
    "        dicts = [json.loads(line)for line in f.readlines()]\n",
    "    return dicts\n",
    "\n",
    "train_dicts = get_dicts(train_file)\n",
    "train_dicts = [p for p in train_dicts if p['Type'] == 'Implicit']\n",
    "sample = train_dicts[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Comparison',\n",
       " 'Comparison.Concession',\n",
       " 'Comparison.Contrast',\n",
       " 'Contingency',\n",
       " 'Contingency.Cause',\n",
       " 'Contingency.Cause.Reason',\n",
       " 'Contingency.Cause.Result',\n",
       " 'Contingency.Condition',\n",
       " 'Expansion',\n",
       " 'Expansion.Alternative',\n",
       " 'Expansion.Alternative.Chosen alternative',\n",
       " 'Expansion.Conjunction',\n",
       " 'Expansion.Exception',\n",
       " 'Expansion.Instantiation',\n",
       " 'Expansion.Restatement',\n",
       " 'Temporal',\n",
       " 'Temporal.Asynchronous.Precedence',\n",
       " 'Temporal.Asynchronous.Succession',\n",
       " 'Temporal.Synchrony']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_senses = [q for p in train_dicts for q in p['Sense']]\n",
    "sorted(set(total_senses))\n",
    "\n",
    "# senses_cnt = Counter('&'.join(p['Sense'])for p in train_dicts)\n",
    "# senses_cnt = dict(sorted(senses_cnt.items()))\n",
    "# senses_cnt\n",
    "\n",
    "# Counter(len(p['Sense'])for p in train_dicts)\n",
    "\n",
    "# Counter(p['Type']for p in train_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
